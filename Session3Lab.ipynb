{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install missing required packages, and upgrade packages to current versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn xgboost shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages used throughout notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import boto3\n",
    "from IPython.display import Image, display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Detection in Images\n",
    "Here we will demonstrate how a pre-trained machine learning model can be used to create value once operationalized. Amazon Rekognition can be used to detect text in an image, where teams from Amazon have already trained Rekognition on millions of images containing text. There are three photos we will try: a simple text based photo, a complex photo with lots of text, and a Nutrien example photo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_quote = 'AI_quote.jpg'\n",
    "times_square = 'times_square.jpg'\n",
    "nutrien_railcar = 'railcar.jpg'\n",
    "\n",
    "print('Simple:')\n",
    "display(Image(f\"{AI_quote}\", width=500))\n",
    "print('\\nComplex:')\n",
    "display(Image(f\"{times_square}\", width=500))\n",
    "print('\\nNutrien:')\n",
    "display(Image(f\"{nutrien_railcar}\", width=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function opens the specified image, parses it using an API call to Amazon Rekognition, then outputs a dataframe with the relevant information returned from the API. Further details are provided in the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_text(photo_file):\n",
    "    \n",
    "    # Setting up the client to make an API request to Amazon Rekognition\n",
    "    client=boto3.client('rekognition')\n",
    "    \n",
    "    # Opening the file into our notebook, then making the API call with the opened image\n",
    "    with open(photo_file, 'rb') as image:\n",
    "        response=client.detect_text(Image={'Bytes': image.read()})\n",
    "    \n",
    "    # Storing the text part of the response from the API\n",
    "    textDetections=response['TextDetections']\n",
    "    \n",
    "    # Setting up a dataframe to store the text found in the API response\n",
    "    text_df = pd.DataFrame(columns=['Text', 'Confidence (%)', 'ID', 'ParentID', 'Type'])\n",
    "    \n",
    "    # Loop that parses all text detected in each image to the created dataframe\n",
    "    for i, text in enumerate(textDetections):\n",
    "            text_df.loc[i, 'Text'] = text['DetectedText']\n",
    "            text_df.loc[i, 'Confidence (%)'] = round(text['Confidence'],2)\n",
    "            text_df.loc[i, 'ID'] = text['Id']\n",
    "            if 'ParentId' in text:\n",
    "                text_df.loc[i, 'ParentID'] = text['ParentId']\n",
    "            text_df.loc[i, 'Type'] = text['Type']\n",
    "    \n",
    "    # Populated dataframe is returned\n",
    "    return text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each example we re-display a sample of the image, then output the returned dataframe that contains the text extracted from each image using the function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo=f\"{AI_quote}\"\n",
    "AI_quote_df = detect_text(photo)\n",
    "display(Image(f\"{AI_quote}\", width=300))\n",
    "AI_quote_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo=f\"{times_square}\"\n",
    "times_square_df = detect_text(photo)\n",
    "display(Image(f\"{times_square}\", width=300))\n",
    "times_square_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo=f\"{nutrien_railcar}\"\n",
    "nutrien_railcar_df = detect_text(photo)\n",
    "display(Image(f\"{nutrien_railcar}\", width=300))\n",
    "nutrien_railcar_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our Nutrien use case, we would like to know the Department of Transportation (DOT) code located just above the table in the image. We loop over all the text in the text column of the dataframe, and use a Regex statement to find text that fits the format of a dot code. More information on regex statements can be found here:\n",
    "https://docs.python.org/3/howto/regex.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for i, text in enumerate(nutrien_railcar_df['Text']):\n",
    "    # findall returns a list of matched strings in each piece of text\n",
    "    found = re.findall('DOT\\s*[A-Za-z0-9]{9}', text)\n",
    "\n",
    "    # if the dot code is found, we assign the dot code to a variable, print, then exit the for loop\n",
    "    if len(found) == 1:\n",
    "        dot_code = found[0]\n",
    "        print(f'{dot_code}')\n",
    "        print(f'Found on line {i}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "The following example is a simple training exercise on carbon emission data (in tonnes) to introduce us to machine learning modelling, using linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "We are going to create a function that splits our data into a training set and a test set. 80% of the data will be used for the training set and the remaining 20% will be used for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def prepare_data(data, target):\n",
    "    # Seperate the predictor variables (X) from the target variable (y) and into their own dataframes\n",
    "    X = data.drop(target, axis=1)\n",
    "    y = data[target]\n",
    "    \n",
    "    # Create a training and test set for the predictor and target variables\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load in our emission data and name the dataframe emissions_data. We will also see what the data looks like by typing in our dataframe name on the line beneath the code to load in the data. This way we can get a snippet of the data to understand what it looks like. Run the cell below to load in the emissions data and see what the data looks like.\n",
    "\n",
    "Emission data can be found here: https://ourworldindata.org/grapher/annual-co2-emissions-per-country\n",
    "\n",
    "Country information data can be found here: https://www.kaggle.com/datasets/fernandol/countries-of-the-world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions_data = pd.read_csv('carbon_emissions.csv')\n",
    "emissions_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Encoding\n",
    "Now we will use One Hot Encoding to convert our categorical variable (Continent) to numerical variables. This is done so the ML model can make numerical sense of the categorical variables. Run the cell below to encode the categorical feature and see what the encoded feature looks like after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "ohe_continent = ohe.fit_transform(emissions_data[['Continent']])\n",
    "ohe_continent_df = pd.DataFrame(ohe_continent, columns=ohe.categories_[0])\n",
    "\n",
    "print('Original:')\n",
    "display(emissions_data[['Continent']])\n",
    "print('One Hot Encoded:')\n",
    "display(ohe_continent_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now append the one hot encoded features to the original dataframe, drop the previous continent feature as it is no longer needed, and also set the row index to the country name for easier use. From here we are ready to model on the final dataframe seen after running the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_emissions_data = pd.concat([emissions_data, ohe_continent_df], axis=1)\n",
    "transformed_emissions_data.drop('Continent', axis=1, inplace=True)\n",
    "transformed_emissions_data.set_index('Country', inplace=True)\n",
    "\n",
    "transformed_emissions_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data\n",
    "Now we will prepare our data by splitting it into training and test sets using the function we made earlier. In order to understand exactly what this function does, we will also see what the X_train, y_train, X_test, y_test datasets in that order. You will notice that the X_train and X_test datasets are all the predictor variables and the y_test and y_train datset is the target variable (emissions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_data(transformed_emissions_data, 'Annual CO2 emissions')\n",
    "display(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Model\n",
    "Now we will run a linear regression model on our prepared dataset below. We will evaluate this model with 4 metrics: mean absolute error, mean squared error, root mean squared error, and r2_score. Run the cell below to create the model, train it, and generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "lr_model = LinearRegression() # create model\n",
    "lr_model.fit(X_train, y_train) # train model\n",
    "lr_pred = lr_model.predict(X_test) # generate predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will show emission prediction outputs vs true values for a specified country in the test set (X_test), using the specified predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_country_outputs(country, model_predictions):\n",
    "    iloc_val = y_test.index.get_loc(country)\n",
    "\n",
    "    print(f\"Input:\\n{X_test.loc[country].name}\\n\")\n",
    "    print(f\"\\tPredicted Carbon Emissions: \\t{model_predictions[iloc_val]:,.0f}\")\n",
    "    print(f\"\\tActual Carbon Emissions: \\t{y_test.loc[country]:,.0f}\")\n",
    "    print(f\"\\tAbsolute error: \\t\\t{abs(model_predictions[iloc_val] - y_test.loc[country]):,.0f}\")\n",
    "    print(f\"\\tSquared error: \\t\\t\\t{(model_predictions[iloc_val] - y_test.loc[country]) ** 2:,.0f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a couple of sample predictions by our model. Run the cell below to look at the input, the predicted emissions, the actual emissions, and the error. Feel free to check other countries, seen in the test portion (X_test) of the split data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_country_outputs('Germany', lr_pred)\n",
    "get_country_outputs('Malaysia', lr_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see for certain countries like Germany and Malaysia, the model predicts quite well within a certain carbon emission range. But others with either really high or really low emissions are poorly handled by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_country_outputs('Brazil', lr_pred)\n",
    "get_country_outputs('Suriname', lr_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a less anecdotal look at the results, let's compute metrics across the entire test set. The cell below will evaluate the model's predictions with the 4 metrics, and print the equation of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(y_test, lr_pred)\n",
    "mse = mean_squared_error(y_test, lr_pred)\n",
    "rmse = mean_squared_error(y_test, lr_pred, squared=False)\n",
    "r2 = r2_score(y_test, lr_pred) # results are nonsense, included for reference\n",
    "\n",
    "coef = lr_model.coef_\n",
    "intercept = lr_model.intercept_\n",
    "cols = X_train.columns\n",
    "\n",
    "print(f\"Mean Absolute Error: {mae:,.0f}\")\n",
    "print(f\"Mean Squared Error: {mse:,.0f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:,.0f}\")\n",
    "print(f\"R2: {r2}\")\n",
    "print(f\"\\nEquation for Regression Model:\")\n",
    "print(f\"log(carbon emissions) = {coef[0]:.2f}({cols[0]}) + {coef[1]:.2f}({cols[1]}) + {coef[2]:.2f}({cols[2]}) + {coef[3]:.2f}({cols[3]}) + {coef[4]:.0f}({cols[4]})\\\n",
    "       + {coef[5]:.2f}({cols[5]}) + {coef[6]:.2f}({cols[6]}) + {coef[7]:.2f}({cols[7]}) + {coef[8]:.2f}({cols[8]}) + {coef[9]:.2f}({cols[9]})\\\n",
    "        + {coef[10]:.2f}({cols[10]}) + {coef[11]:.2f}({cols[11]}) + {coef[12]:.2f}({cols[12]}) + {coef[13]:.2f}({cols[13]}) + {coef[14]:.2f}({cols[14]})\\\n",
    "         + {coef[15]:.2f}({cols[15]}) + {coef[16]:.2f}({cols[16]}) + {coef[17]:.2f}({cols[17]}) + {coef[18]:.2f}({cols[18]}) + {coef[19]:.2f}({cols[19]})\\\n",
    "         + {intercept:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the results by no means are perfect (see R2 score), it is interesting to see how certain features push the predictions one way or the other. Depending what continent the country is on influences whether the model thinks that country will have more or less carbon emissions for example.\n",
    "\n",
    "To achieve stronger results we could try various other models, scaling, and hyperparameter tuning. Ideally the dataset could also contain stronger correlated features like industry metrics and vehicle usage, more directly related to carbon emissions. For linear regression we would likely also want to remove outliers like China and India, whereas other models would likely handle these better. Further exploration, implementing some of the noted improvements, will be done on the emissions dataset at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification (Credit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Rule\n",
    "We want to understand what the accuracy of a Naive Rule Model is, so we create a simple function to get us the accuracy for it. This is a simple and effective way to rule out any ML model that does not make value adding predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_rule_accuracy(y_train, y_test):\n",
    "    majority_class = y_train.value_counts().idxmax()\n",
    "\n",
    "    test_counts = y_test.value_counts()\n",
    "    accuracy_naive = test_counts[majority_class] / test_counts.sum()\n",
    "\n",
    "    print(f\"The accuracy of the Naive Model is: {accuracy_naive}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "We want to create a function to automatically evaluate our models. We will be looking at accuracy, recall, percision, f1-score, confusion matrix and the ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, ConfusionMatrixDisplay, RocCurveDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate(model, X_test, y_test):\n",
    "    pred = model.predict(X_test)\n",
    "    # accuracy = correct_predictions / all_predictions \n",
    "    acc = accuracy_score(y_test, pred)\n",
    "\n",
    "    # true_positives / (true_positives + false_postives)\n",
    "    # how many positive predictions were true\n",
    "    prec = precision_score(y_test, pred, average='weighted')\n",
    "\n",
    "    # true_postives / (true_positives + false_negatives)\n",
    "    # how many postives out of all were identified\n",
    "    rec = recall_score(y_test, pred, average='weighted')\n",
    "\n",
    "    # harmonic mean of precision and recall\n",
    "    f1 = f1_score(y_test, pred, average='weighted')\n",
    "    \n",
    "    print(f\"accuracy: {acc}\")\n",
    "    print(f\"precision: {prec}\")\n",
    "    print(f\"recall: {rec}\")\n",
    "    print(f\"f1: {f1}\")\n",
    "    \n",
    "    try:\n",
    "        # prob = model.predict_proba(X_test)\n",
    "        # roc_auc = roc_auc_score(y_test, prob, multi_class='ovo')\n",
    "        # print(f\"roc_auc: {roc_auc}\")\n",
    "        roc_display = RocCurveDisplay.from_estimator(model, X_test, y_test)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    cm_display = ConfusionMatrixDisplay.from_predictions(y_test, pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "We will now look at credit data for another Binary Classification problem. We will load in the data as credit_data, veiw it and then split it similarly to the rice dataset. For this dataset we will be looking at payment history patterns for customers (the CustomerID field has been removed for anonymity) and try to predict if they will be credit risks or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data = pd.read_csv('Company.csv')\n",
    "credit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_data(credit_data, 'Risk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Rule Benchmark\n",
    "Before we do any ML, lets look at the Naive Model accuracy. If a model cant beat the accuracy of the Naive Model, then there is no point in looking at it further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_rule_accuracy(y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model\n",
    "Now lets run the model, like we did above and see if we get an improved output from the naive rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "evaluate(model, X_test, y_test)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.xaxis.set_ticklabels(['Low Risk','High Risk'])\n",
    "ax.yaxis.set_ticklabels(['Low Risk','High Risk'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to see how many false positives and false negatives there were"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)\n",
    "false_positives = 0\n",
    "false_negatives = 0\n",
    "for prediction, truth in zip(pred, y_test):\n",
    "    if truth == 1 and prediction == 0:\n",
    "        false_negatives += 1\n",
    "    if truth == 0 and prediction == 1:\n",
    "        false_positives += 1\n",
    "\n",
    "print(f\"False Positives: {false_positives}\")\n",
    "print(f\"False negatives: {false_negatives}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification (Rice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation\n",
    "Like all the previous dataset we will load in our rice data as rice_data. We will use this dataset to predict if the rice is Jasmine or is Gonen. (1 = Jasmine, 0 = Gonen). We will load in the data and then split it into training and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rice_data = pd.read_csv('rice.csv')\n",
    "rice_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_data(rice_data, 'Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Rule Benchmark\n",
    "Before we do any ML, lets look at the Naive Model accuracy. If a model cant beat the accuracy of the Naive Model, then there is no point in looking at it further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_rule_accuracy(y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model\n",
    "Now lets run a Logistic Regression Model and produce some evaluation metrics and the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "evaluate(model, X_test, y_test)\n",
    "\n",
    "fig = plt.gcf()\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.xaxis.set_ticklabels(['Gonen','Jasmine']); ax.yaxis.set_ticklabels(['Gonen','Jasmine']);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below display's the ROC_AUC score and graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification (Crop Recommendation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Now we will import the crop.csv dataset as crop_data. We will use this dataset to better predict the 'label' colunm. Run the cell below to see what the dataset looks like after it has been loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_data = pd.read_csv('crops.csv')\n",
    "crop_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the heart dataset, we will now split this dataset into training and test sets. If you would like to see what these datasets look like, run the cell, open another cell below them and type in the name(s) of the dataset(s) you wish to see (see similar example with heart_data above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_data(crop_data, 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Rule Benchmark\n",
    "Before we do any ML, lets look at the Naive Model accuracy. If a model cant beat the accuracy of the Naive Model, then there is no point in looking at it further. The Naive Rule Benchmark for this problem will be very low, given it is a multi-class problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_rule_accuracy(y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Model\n",
    "We will first use the Naive Bayes Model on our dataset. We are using the Gaussian Naive Bayes Model as our predictor variables are continous and not discrete. Click the cell below to run it and get a confusion matrix, as well as the accuracy, percision, recall, f1 score and roc_auc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "evaluate(model, X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sochastic Gradient Descent Model\n",
    "Now we will be running the Sochastic Gradient Descent Model. Click the cell below to produce the output and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "model = SGDClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "evaluate(model, X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Model\n",
    "Now we will be running the Sochastic Gradient Descent Model. Click the cell below to produce the output and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "model = Perceptron()\n",
    "model.fit(X_train, y_train)\n",
    "evaluate(model, X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Model\n",
    "Now we will be running a Decision Tree Model. Click the cell below to produce the output and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "evaluate(model, X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Model\n",
    "Now we will be running a XGBoost Model. Click the cell below to produce the output and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "evaluate(model, X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "We will use python to optimize the hyperparameters of our SGD Classifier. We want to see if through hyperparameter tuning we can improve the performance of the model. We will be using the same crop dataset as we used for the first SGD Model, so we will start by splitting the dataset again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_data(crop_data, 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us write in the hyperparameter tuning function. In the param_grid we will be defining the various parameters we discussed in the slide. Run the 2 cells below to tune the model hyperparameters, the cells following will display the results of the tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    \"penalty\": ['l1', 'l2', 'elasticnet'], # The various options to put a penalty on errors (also known as regularization)\n",
    "    \"alpha\": [0.0001, 0.001, 0.01], # The constant that multiplies the regularization term. The higher the value the, the stronger the penalty\n",
    "    \"eta0\": [0.001, 0.01, 0.1], # The initial learning rate for the model. Will change with adaptive learning\n",
    "    \"learning_rate\": ['constant', 'adaptive'] # Does the model keep the learning rate constant or change as it runs \n",
    "}\n",
    "grid_cv = GridSearchCV(SGDClassifier(), param_grid, n_jobs=-1, cv=5, scoring=\"f1_weighted\")\n",
    "# n_jobs = means the number of jobs to run in parallel, -1 means use all processors\n",
    "# cv = cross validation, how many folds\n",
    "# scoring = what we will be scoring the model on, in our case it will be the weighted f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to see what is the best score produced by the optimal set of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to find out which combination of hyperparameters turned out to be the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run both the cells below to evaluate the model with the optimal set of hyperparameters and see what the performance stats look like above. You will notice a significant improvement in the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = grid_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling\n",
    "We want to see if our model preforms any better if we standardize or normalize the data. Just like before we will be using our crop data and splitting into training and test sets. We will be using the same SGD classifier because that model had some room for improvement. We want to see if either standardization or normalization will improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep\n",
    "First step is to split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_data(crop_data, 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will scale the data. We are going to both normalize the data and standardize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "X = crop_data.drop(\"label\", axis=1)\n",
    "y = crop_data[\"label\"]\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "standard_scaler.fit(X)\n",
    "X_s_scaled = pd.DataFrame(standard_scaler.transform(X), columns=X.columns)\n",
    "\n",
    "minmax_scaler = MinMaxScaler()\n",
    "minmax_scaler.fit(X)\n",
    "X_mm_scaled = pd.DataFrame(minmax_scaler.transform(X), columns=X.columns)\n",
    "with pd.option_context('display.float_format', lambda x: '%.3f' % x):  \n",
    "    print(\"Unscaled Data:\") \n",
    "    display(X.describe())\n",
    "    print(\"Standardized Data:\")\n",
    "    display(X_s_scaled.describe())\n",
    "    print(\"Normalized Data:\")\n",
    "    display(X_mm_scaled.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unscaled Data\n",
    "We are going to run the same model on the three datasets above and see which one comes out with the best performance. All of them are SGD Models and we will see the confusion matrix, accuracy, percision, recall and the f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = SGDClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "evaluate(model, X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_s_scaled, y, test_size=0.2, random_state=42)\n",
    "model = SGDClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "evaluate(model, X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_mm_scaled, y, test_size=0.2, random_state=42)\n",
    "model = SGDClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "evaluate(model, X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP Values\n",
    "To understand the importance of predictor variable will have on the outcome, we can use the SHAP package in python. We want to know how the predictors affect the outcome for our crop dataset with an XGBoost model so we will first train an XGBoost Model with that data again then see the shap values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training and test sets, like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = prepare_data(crop_data, 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now train and evaluate the model, same as we did once before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "y_train_enc = encoder.transform(y_train)\n",
    "y_test_enc = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier()\n",
    "\n",
    "model.fit(X_train, y_train_enc)\n",
    "evaluate(model, X_test, y_test_enc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the two cell below to calculate the SHAP values. It may take a couple minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(model, X_train)\n",
    "shap_values = explainer(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to see a bar plot of the SHAP values for each outcome. You will notice that after a comma there is a number from 0 - 21. That number represents one of the 22 outcomes the target variable could take and the graph will represent the importance of the features that will lead to that specific outcome. To see how the SHAP values change with each target class, change the number from what is with anything in between 0 and 21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values[...,0], show=False)\n",
    "fig = plt.gcf()\n",
    "ax = plt.gca()\n",
    "fig.set_figheight(11)\n",
    "fig.set_figwidth(11)\n",
    "font_dict = {'size':16}\n",
    "font_dict_title = {'size':18}\n",
    "fig.patch.set_facecolor('xkcd:light grey')\n",
    "plt.xlabel('Mean Shap Value for Target Variable',font_dict)\n",
    "plt.ylabel('Predictor Variables', font_dict)\n",
    "plt.title('SHAP Values for Crop Classification', font_dict_title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emissions Revisited\n",
    "Now that we have seen some techniques to improve machine learning models, lets try implementing some of them on the initial linear regression example on carbon emissions, focusing on two new models: RandomForestRegressor and XGBRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions_data = pd.read_csv('carbon_emissions.csv')\n",
    "emissions_data.set_index('Country', inplace=True)\n",
    "emissions_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Encoding and Scaling\n",
    "Now we will use One Hot Encoding to convert our categorical variable (Continent) to numerical variables and scaling will be done on the numerical values. To easily do both in one step we will use a column transformer, which handles the dataframe transformations more direct. Run the cell below to encode the categorical features, scale the numeric features, and see what the dataset looks like after. Note column names are updated for their transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "all_features = emissions_data.columns.tolist()\n",
    "categorical_features = ['Continent']\n",
    "passthrough_columns = ['Annual CO2 emissions']\n",
    "\n",
    "# list comprehension to get all column names except categorical and passthrough (target)\n",
    "numerical_features = [features for features in all_features if features not in passthrough_columns + categorical_features] \n",
    "\n",
    "\n",
    "ct = ColumnTransformer(transformers=[(\"scaled\", StandardScaler(), numerical_features),\n",
    "                                     (\"onehot\", OneHotEncoder(sparse=False), categorical_features)],\n",
    "                                    remainder='passthrough')\n",
    "\n",
    "transformed_emissions_data = ct.fit_transform(emissions_data)\n",
    "transformed_emissions_data = pd.DataFrame(transformed_emissions_data, columns = ct.get_feature_names_out(), index = emissions_data.index)\n",
    "\n",
    "transformed_emissions_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_data(transformed_emissions_data, 'remainder__Annual CO2 emissions')\n",
    "display(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regression Grid Search\n",
    "Now we will run a grid search over a random forest regression model on our prepared dataset. We will evaluate this model with 4 metrics: mean absolute error, mean squared error, root mean squared error, and R2 score. Run the cell below to create the model as well as train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [5, 50, 100, 150, 250], # The number of trees in the forest\n",
    "    \"max_depth\": [None, 2, 5, 10, 15],# The possible depth of each individual tree\n",
    "}\n",
    "grid_cv = GridSearchCV(RandomForestRegressor(random_state=5), param_grid, n_jobs=-1, cv=5, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "grid_cv.fit(X_train, y_train) # train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we recreate the RFR model using the best result from the grid search. Then generate predictions using that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_model = grid_cv.best_estimator_ # create model\n",
    "rfr_model.fit(X_train, y_train)\n",
    "rfr_pred = rfr_model.predict(X_test) # generate predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the same sample predictions from earlier, but this time use our random forest regression model. Run the cell below to look at the input, the predicted emissions, the actual emissions, and the error. Feel free to check other countries, seen in the test portion of the split data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_country_outputs('Germany', rfr_pred)\n",
    "get_country_outputs('Malaysia', rfr_pred)\n",
    "get_country_outputs('Brazil', rfr_pred)\n",
    "get_country_outputs('Suriname', rfr_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will evaluate the model with the 4 metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(y_test, rfr_pred)\n",
    "mse = mean_squared_error(y_test, rfr_pred)\n",
    "rmse = mean_squared_error(y_test, rfr_pred, squared=False)\n",
    "r2 = r2_score(y_test, rfr_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error: {mae:,.0f}\")\n",
    "print(f\"Mean Squared Error: {mse:,.0f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:,.0f}\")\n",
    "print(f\"R2: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With some relatively simple changes our RMSE is about a quarter of the earlier linear regression model's RMSE, and the R2 score has become actually usable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Grid Search\n",
    "We can repeat the previous process for an XGBoost Regression model instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'booster': ['gbtree', 'dart'],\n",
    "    \"n_estimators\": [1, 2, 3, 4, 5, 50, 100], # The number of trees in the forest\n",
    "    \"max_depth\": [None, 2, 5, 10],# The possible depth of each individual tree\n",
    "}\n",
    "grid_cv = GridSearchCV(xgb.XGBRegressor(random_state=5), param_grid, n_jobs=-1, cv=5, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "grid_cv.fit(X_train, y_train) # train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = grid_cv.best_estimator_\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_country_outputs('Germany', xgb_pred)\n",
    "get_country_outputs('Malaysia', xgb_pred)\n",
    "get_country_outputs('Brazil', xgb_pred)\n",
    "get_country_outputs('Suriname', xgb_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(y_test, xgb_pred)\n",
    "mse = mean_squared_error(y_test, xgb_pred)\n",
    "rmse = mean_squared_error(y_test, xgb_pred, squared=False)\n",
    "r2 = r2_score(y_test, xgb_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error: {mae:,.0f}\")\n",
    "print(f\"Mean Squared Error: {mse:,.0f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:,.0f}\")\n",
    "print(f\"R2: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see an improvement using XGBoost over the Random Forest Regressor! There are also many more hyperparameters we could try tuning both for the RFR and XGBoost, with XGBoost having even more. If we were to continue this comparison we would want to actively compare the grid_cv.best_score_ 's, rather than the test set RMSE so as not to create a bias towards the test set and keep the model as generalizable as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "57980723ff7ead57ef7e7ea758f7e14d60942de9f794a6d053ff3ad8e641bdcf"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
